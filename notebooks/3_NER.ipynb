{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3 : application du modèle de NER, conversion en tableau CSV, et géolocalisation des adresses\n",
    "\n",
    "Le notebook précédent nous a apporté comme données de sorties le texte extrait des images des colonnes, se trouvant dans `/sample_1951/pero`, dans divers formats (PNG, TXT, XML et XML-ALTO). Nous avons converti les XML en question en un unique fichier JSON-L grace au script `/notebooks_1951/XmlToJson.py` (attention à ne pas exporter les fichiers XML et XML-ALTO).\n",
    "\n",
    "Nous avons procédé à l'annotation de 15 colonnes (c'est-à-dire 1160 lignes) pour le NER avec l'outil **Prodigy**. Nous avons ensuite utilisé Spacy pour entrainer un modèle de NER à partir de ces annotations : ce modèle se trouve à `modeles_1951/3_Prodigy/model-best`. Nous avons en réalité entrainé plusieurs modèles et comparé leurs performances (cf. le mémoire pour plus de détails)\n",
    "\n",
    "Nous partons donc des données des annuaires en format JSON-L. Nous appliquons dans un premier temps le modèle de NER que nous avons entrainé (qui se trouve ici : `/modeles_1951/2_Prodigy/model-best`). Nous allons ensuite associer des coordonnées géographiques à chaque adresse, en utilisant le géocodeur du gouvernement. Finalement, nous convertissons ces données en tableau, ou les colonnes sont les étiquettes du NER, c'est-a-dire : le nom de la personne, son adresse, etc. Nous faisons deux tableaux différents : un ou une ligne du tableau correspond à un propriétaire (et son ou ses immeubles), et un ou une ligne correspond à un immeuble parisien. Nous exportons ensuite ces tableaux en format CSV.\n",
    "- input : le contenu de l'annuaire (format : JSON-L)\n",
    "- output : deux tableaux CSV :\n",
    "    - un premier où une ligne = un propriétaire ;\n",
    "    - un deuxième où une ligne = un immeuble.\n",
    "\n",
    "Les temps indiqués sont pour le traitement des 531 pages de l'annuaire des propriétaires.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "Nous nous servons d'un environnement avec :\n",
    "- python version 3.10.14\n",
    "- pandas version 2.1.3\n",
    "- numpy version 1.26.4\n",
    "- geopandas version 0.14.0\n",
    "- requests version 2.31.0\n",
    "- spacy version 3.7.4\n",
    "- folium version 0.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape I : Application du NER ligne par ligne et mise en forme des entrées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Import des librairies, définition des chemins, et import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaron/anaconda3/envs/memoire/lib/python3.10/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# import des libraries\n",
    "\n",
    "# pour le NER\n",
    "import spacy\n",
    "import json\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# pour les dataframes et geodataframes\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "from shapely.geometry import Polygon, LineString, Point\n",
    "\n",
    "# Libraries pour le geocodeur\n",
    "import ast\n",
    "import requests\n",
    "import io\n",
    "import os\n",
    "\n",
    "# Pour les cartes\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from folium import Map\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Desactiver le nombre max de colonnes\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Pour montrer les images\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aaron/Documents/M2/memoire_M2/0_donnees_rangees\n",
      "/home/aaron/Documents/M2/memoire_M2/0_donnees_rangees/liste_adresses_1951/6_liste_rues_1951.csv\n"
     ]
    }
   ],
   "source": [
    "# Définition des paths intéressants\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "print(parent_dir)\n",
    "path_df_rues = os.path.join(parent_dir, 'liste_adresses_1951/6_liste_rues_1951.csv')\n",
    "print(path_df_rues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Chargement du dataframe qui associe chaque code rue au nom de la rue explicite. Dataframe propre car relu à la main.\n",
    "df_codes = pd.read_csv(path_df_rues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# On définit aussi une liste avec les étiquettes du NER (pour créer le CSV)\n",
    "fieldnames = [\"PER\", \"PRENOM\", \"STATUT\", \"ORG\", \"NUM\", \"TYPE_VOIE\", \"NOM_VOIE\", \"ARR\", \"VILLE\", \"LOC\", \"PART\", \"CODES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Définition et application de la fonction convertissant le JSON-L en CSV\n",
    "\n",
    "Cette fonction prend en entrée des données textuelles (ici le contenu d'un annuaire de propriétaires), y applique un modèle de NER, puis crée un tableau CSV avec ces informations.\n",
    "\n",
    "Les colonnes du CSV correspondent aux étiquettes du NER.\n",
    "\n",
    "Dans le fichier CSV de sortie, une ligne correspond à une entrée de l'annuaire. Nous définissons le début d'une entrée de l'annuaire la première étiquette qui ne soit pas un CODES mais soit juste après un CODE. Et nous définissons la fin d'une entrée comme la dernière étiquette \"CODES\" (c'est-à-dire la première étiquette CODES qui ne soit pas suivie d'un CODES). Le but de cette définition est de prendre en compte tous les codes d'une personne dans une seule entrée, si ils sont répartis sur plusieurs lignes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def json_2_csv_v2(input_path, output_path, model_path, fieldnames):\n",
    "\n",
    "    '''\n",
    "    Cette fonction prend en entrée des données textuelles (ici le contenu d'un annuaire de propriétaires),\n",
    "    y applique un modèle de NER, puis crée un tableau CSV avec ces informations.\n",
    "    Les colonnes du CSV correspondent aux étiquettes du NER.\n",
    "    - input_path > path des données d'entrée. Doit etre de format JSON-L\n",
    "    - output_path > path des données de sorties. Doit etre de format CSV\n",
    "    - model_path > path du modele de NER Prodigy qu'on va utiliser\n",
    "    - fieldnames > liste des étiquettes du NER. Correspondra aux colonnes du CSV.\n",
    "    '''\n",
    "\n",
    "    # Chargement des données\n",
    "    with open(input_path, 'r', encoding='utf-8') as input_file:\n",
    "        input_data = [json.loads(line) for line in input_file]\n",
    "\n",
    "    # Chargement du modele Prodigy\n",
    "    nlp = spacy.load(model_path)\n",
    "\n",
    "\n",
    "\n",
    "    # Etape 1 : On applique le NER ligne par ligne\n",
    "    liste_output_prodigy = []\n",
    "\n",
    "    for input_item in input_data:\n",
    "\n",
    "        # On crée un dictionnaire de sortie. Ce dico va garder les info de texte et metadonnes du jsonl, et rajouter les infos du NER\n",
    "        dico_output = {}\n",
    "        dico_prodigy_definitif = defaultdict(list)\n",
    "\n",
    "        # On va chercher les éléments qui nous intéressent\n",
    "        text = input_item['text']\n",
    "        meta = input_item['meta']\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Dico temporaire ou on rajoute les couples texte-étiquette classés par le NER.\n",
    "        # .append permet de conserver toutes les informations, quand une étiquette est associée à plusieurs chaines de\n",
    "        # caractères dans une seule ligne\n",
    "        dico_prodigy = defaultdict(list)\n",
    "        for ent in doc.ents:\n",
    "            dico_prodigy[ent.label_].append(ent.text)\n",
    "\n",
    "        # Mais maintenant dans ce dictionnaire, chaque étiquette est associée à une liste\n",
    "        # Des fois, cette liste ne contient qu'un seul élément\n",
    "        # On va remplir un deuxième dictionnaire mais ne faire des listes que quand il y a effectivement plusieurs éléments,\n",
    "        # sauf pour CODES ou c'est plus pratique de garder la structure de liste pour toutes les lignes\n",
    "        for key, value in dico_prodigy.items():\n",
    "            if key != \"CODE\":\n",
    "                if len(value) == 1:\n",
    "                    dico_prodigy_definitif[key] = value[0]\n",
    "\n",
    "                if len(value) > 1:\n",
    "                    dico_prodigy_definitif[key] = value\n",
    "\n",
    "\n",
    "            # On fait en sorte que la colonne \"CODES\" contienne une liste de tous les CODES détectés par le NER\n",
    "            \n",
    "            if key == 'CODE':\n",
    "                codes = []\n",
    "                                        \n",
    "                for index, item in enumerate(value):\n",
    "                    codes.append(item)\n",
    "\n",
    "                dico_prodigy_definitif[\"CODES\"] = codes\n",
    "\n",
    "        \n",
    "        # On rajoute les informations qu'on veut dans le dico de sortie de cette ligne :\n",
    "        # Les couples texte-étiquettes qu'on a trouvés et mis en forme plus tot, le texte original, les métadonnées\n",
    "\n",
    "        dico_output['prodigy'] = dico_prodigy_definitif\n",
    "        dico_output['text'] = text\n",
    "        dico_output['meta'] = meta\n",
    "\n",
    "\n",
    "        # Et on rajoute ce dictionnaire de sortie de la ligne dans une liste qui contiendra tous ces dictionnaires\n",
    "\n",
    "        liste_output_prodigy.append(dico_prodigy_definitif)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Etape 2 : On reconstitue les entrées, pour avoir un dictionnaire = une entrée de l'annuaire (et plus = une ligne de l'annuaire)\n",
    "    # une entrée = les étiquettes apres un code et jusqu'au code d'après.\n",
    "    \n",
    "    # Initialisation :\n",
    "    liste_entrees = []\n",
    "\n",
    "    code_n_1 = False\n",
    "    code_n = False\n",
    "\n",
    "    temp_dict = defaultdict(list)\n",
    "\n",
    "\n",
    "    # On itère sur les items de la liste des dicos des lignes\n",
    "    # Si on a un code dans l'entrée n-1 et pas dans l'entrée n, on le rajoute à la liste car c'est comme ca que je définis une entrée\n",
    "    # Sinon, on rajoute uniquement l'item dans un dictionnaire temporaire\n",
    "\n",
    "    for item in liste_output_prodigy:\n",
    "\n",
    "        # On récupère l'info : est-ce que cet élément à pour étiquette CODES ?\n",
    "        if \"CODES\" in item:\n",
    "            code_n = True\n",
    "\n",
    "        else:\n",
    "            code_n = False\n",
    "\n",
    "        # Cas 1 : n-1 est un CODES et n n'est pas un code\n",
    "        # alors, la ligne n-1 est la dernière ligne d'une entrée\n",
    "        # Dans ce cas, on rajoute les informations stockées dans le dico temporaire dans la liste des entrées\n",
    "        # On récupère aussi la ligne n et on la rajoute dans le dico temporaire après l'avoir réinitialisé\n",
    "        if (code_n_1 == True) and (code_n == False):\n",
    "            liste_entrees.append(temp_dict)\n",
    "            temp_dict = defaultdict(list)\n",
    "\n",
    "            # Code pour rajouter correctement l'entrée dans la liste des entrées\n",
    "            for item_key, item_value in item.items():\n",
    "                # Cas A : la clef n'existe pas > pas de précaution nécessaire\n",
    "                if item_key not in temp_dict.keys():\n",
    "                    temp_dict[item_key] = item_value\n",
    "\n",
    "                # Cas B : la clef existe\n",
    "                else:\n",
    "                    # On traite differemment \"code\" et le reste parce que on veut une grande liste avec tous les codes pour code,\n",
    "                    # et pour les autres on veut juste les rajouter dans la liste\n",
    "\n",
    "                    # Sous-cas 1 : Cette valeur est déjà une liste\n",
    "                    if isinstance(temp_dict[item_key], list):\n",
    "                        if item_key == \"CODES\":\n",
    "                            temp_dict[item_key].extend(item_value)\n",
    "                        else:\n",
    "                            temp_dict[item_key].append(item_value)\n",
    "\n",
    "                    # Sous-cas 2 : Cette valeur n'est pas une liste                        \n",
    "                    else:\n",
    "                        temp_dict[item_key] = [temp_dict[item_key]]\n",
    "                        if item_key == \"CODES\":\n",
    "                            temp_dict[item_key].extend(item_value)\n",
    "                        else:\n",
    "                            temp_dict[item_key].append(item_value)\n",
    "                        \n",
    "\n",
    "        # Cas 2 : nous n'avons pas \"n-1 est un CODES et n n'est pas un code\"\n",
    "        # cas ou n-1 n'est pas la dernière ligne d'une entrée\n",
    "        # On utilise un code semblable pour rajouter le contenu de la ligne n-1 dans le dico temporaire\n",
    "        else:\n",
    "            for item_key, item_value in item.items():\n",
    "                if item_key not in temp_dict.keys():\n",
    "                    temp_dict[item_key] = item_value\n",
    "                else:\n",
    "                    if isinstance(temp_dict[item_key], list):\n",
    "                        if item_key == \"CODES\":\n",
    "                            temp_dict[item_key].extend(item_value)\n",
    "                        else:\n",
    "                            temp_dict[item_key].append(item_value)\n",
    "\n",
    "                    else:\n",
    "                        temp_dict[item_key] = [temp_dict[item_key]]\n",
    "                        if item_key == \"CODES\":\n",
    "                            temp_dict[item_key].extend(item_value)\n",
    "                        else:\n",
    "                            temp_dict[item_key].append(item_value)\n",
    "\n",
    "\n",
    "        # On fait passer les informations \"est-ce que les lignes n et n-1 sont des CODES ?\"\n",
    "        # de n a n+1\n",
    "        code_n_1 = code_n\n",
    "        code_n = None\n",
    "\n",
    "    # Cas 3 : c'est la dernière ligne de l'annuaire\n",
    "    if item == liste_output_prodigy[-1]:\n",
    "        liste_entrees.append(temp_dict)\n",
    "\n",
    "\n",
    "\n",
    "    # Etape 3 : On sauvegarde ces informations dans le CSV de sortie\n",
    "\n",
    "    with open(output_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in liste_entrees:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    return liste_entrees\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaron/.local/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.4.1 and may not be 100% compatible with the current version (3.7.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Application à nos données\n",
    "\n",
    "input_path = os.path.join(parent_dir, 'sample_1951/contenu_echantillon.jsonl')\n",
    "output_path = os.path.join(parent_dir, 'sample_1951/contenu_echantillon.csv')\n",
    "model_path = os.path.join(parent_dir, 'modeles_1951/2_Prodigy/model-best')\n",
    "\n",
    "liste_entrees = json_2_csv_v2(input_path, output_path, model_path, fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# > 4 min 30 de traitement (pour l'ensemble de l'annuaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 2 : utiliser ces resultats pour produire deux autre fichiers CSV :\n",
    "- un CSV où une ligne = un étiquette CODES (c'est-a-dire \"une rue : un ou plusieurs immeubles\")\n",
    "- un CSV où un immeuble = une ligne (très utile pour faire des cartes sur le nombre d'immeubles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Conversion en dataframes des données, création d'un index propriétaire, et d'une colonne comptabilisant le nombre de codes par entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Conversion des données en dataframe\n",
    "df_proprios = pd.DataFrame(liste_entrees, columns=fieldnames)\n",
    "# Normalisation des noms de colonnes (pour les mettre en minuscules)\n",
    "dico_fieldnames = {item : item.lower() for item in fieldnames}\n",
    "df_proprios = df_proprios.rename(dico_fieldnames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Je cree un index pour le df, chaque propriétaire aura un index\n",
    "index_total_data = pd.Series(range(0,len(df_proprios)))\n",
    "df_proprios.insert(0, 'index_total', index_total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Je créé une colonne \"nb_codes\" qui compte le nombre de codes pour chaque entree\n",
    "df_proprios['nb_codes'] = df_proprios['codes'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Définition et application d'une fonction pour récupérer les codes séparés sur deux lignes\n",
    "On constate que certaines entrée ont des codes séparées sur deux lignes, de type '4923 :', '242'. Nous allons le solidariser en une seule chaine de caractères. Cette fonction fonctionne jusqu'à 8 numéros de rues différents (par exemple : '4923 :', '242, 244, 246, [...], 256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def fix_codes(row):\n",
    "    # On récupère tous les codes de l'entrée dans une liste\n",
    "    liste_codes = row.codes\n",
    "    nvelle_liste = []\n",
    "\n",
    "    code_content_n = \"\"\n",
    "    code_content_n_1 = \"\"\n",
    "   \n",
    "    # Cas particulier pour un seul element\n",
    "    # Pas besoin de vérifier, le code est sur une seule ligne\n",
    "    if len(liste_codes) == 1:\n",
    "        nvelle_liste = liste_codes\n",
    "\n",
    "    # Si n = 2 ou plus, c'est-a-dire si le code est sur + d'une ligne\n",
    "    else:\n",
    "        for index_code, code_content in enumerate(liste_codes):\n",
    "            code_content_n = code_content\n",
    "            \n",
    "            regex_1 = r'\\d{3,4}( )?:( \\d{1,2}( bis)?( ter)?(,)?)?( \\d{1,2}( bis)?( ter)?(,)?)?( \\d{1,2}( bis)?( ter)?(,)?)?'\n",
    "            regex_2 = r'\\d{1,3}( bis)?( ter)?([,\\.])?( \\d{1,3}( bis)?( ter)?)?([,\\.] \\d{1,3}( bis)?( ter)?)?([,\\.] \\d{1,3}( bis)?( ter)?)?([,\\.] \\d{1,3}( bis)?( ter)?)?([,\\.] \\d{1,3}( bis)?( ter)?)?([,\\.] \\d{1,3}( bis)?( ter)?)?([,\\.] \\d{1,3}( bis)?( ter)?)?(\\.)?'\n",
    "\n",
    "            # Cas 1 : On a effectivement un code sur deux lignes de type : '4923 :', '242'\n",
    "            if (re.fullmatch(regex_1, code_content_n_1)) and (re.fullmatch(regex_2, code_content_n)):\n",
    "                bon_code = f\"{code_content_n_1} {code_content_n}\"\n",
    "                nvelle_liste.append(bon_code)\n",
    "                code_content_n_1 = \"\"\n",
    "                code_content_n = \"\"\n",
    "\n",
    "            # Cas 2 : Dernier code : ne peut pas avoir un code sur deux lignes avec la ligne d'apres\n",
    "            elif index_code == (len(liste_codes)-1):\n",
    "                if code_content_n_1 != \"\":\n",
    "                    nvelle_liste.append(code_content_n_1)\n",
    "                if code_content_n != \"\":\n",
    "                    nvelle_liste.append(code_content_n)\n",
    "\n",
    "            # Cas 3 : sinon\n",
    "            else:\n",
    "                if code_content_n_1 != \"\":\n",
    "                    nvelle_liste.append(code_content_n_1)\n",
    "\n",
    "                code_content_n_1 = code_content_n\n",
    "                code_content_n = \"\"\n",
    "  \n",
    "    return nvelle_liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_proprios['nveaux_codes'] = df_proprios.apply(fix_codes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 0.5 sec de traitement (pour toutes les données)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "## 3. Création d'une deuxième dataframe ou une ligne = un code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# On copie le dataframe\n",
    "df_bis = df_proprios\n",
    "\n",
    "# On sauvegarde la colonne codes\n",
    "df_bis['code'] = df_bis['nveaux_codes']\n",
    "\n",
    "# Puis on splite\n",
    "df_bis = df_bis.explode('code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 0.1 sec (pour toutes les données)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "## 4. Définition et application de la fonction qui transforme les codes en adresses explicites\n",
    "Fonction qui extrait la voie explicite qui correspond au code de la voie, et le ou les plusieurs numéros possédés dans cette rue\n",
    "\n",
    "Par ex, si le code est \"4923 : 109, 111\", le code va :\n",
    "- dire que \"4923\" correspond au \"boulevard Voltaire\" grace au df des rues importé plus haut\n",
    "- découper les codes \"109\" et \"111\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pattern = r'(\\d{1,4})[^\\d]*(\\d{1,3})([^\\d]*(\\d{1,3}))?([^\\d]*(\\d{1,3}))?([^\\d]*(\\d{1,3}))?([^\\d]*(\\d{1,3}))?'\n",
    "\n",
    "liste_erreurs = []\n",
    "\n",
    "def d_code_voie(row):\n",
    "    code = row['code']\n",
    "    match = re.search(pattern, code)\n",
    "    if match:\n",
    "        # Si on trouve bien ce pattern\n",
    "        # Alors on recupere les infos : code de la rue, numero/s d'immeuble/s\n",
    "        groups = [match.group(1), match.group(2), match.group(4), match.group(6), match.group(8), match.group(10)]\n",
    "        code_voie = groups[0]\n",
    "        numeros = groups[1:]\n",
    "\n",
    "        # On enleve aussi les None causes par le regex\n",
    "        numeros = [x for x in numeros if x is not None]\n",
    "\n",
    "        # Conversion code voie > nom explicite de la voie\n",
    "        try:\n",
    "            df_codes.loc[df_codes['code'] == code_voie, 'nom_voie'].values[0]\n",
    "        except:\n",
    "            liste_erreurs.append([row, code_voie])\n",
    "        \n",
    "        else:\n",
    "            nom_voie = df_codes.loc[df_codes['code'] == code_voie, 'nom_voie'].values[0]\n",
    "            type_voie = df_codes.loc[df_codes['code'] == code_voie, 'type_voie'].values[0]\n",
    "            arr = df_codes.loc[df_codes['code'] == code_voie, 'arr'].values[0]\n",
    "\n",
    "            return code_voie, numeros, len(numeros), type_voie, nom_voie, arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Création d'un troisieme dataframe ou une ligne = un immeuble (ce qui nous intéresse pour l'index par immeuble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_ter = df_bis\n",
    "\n",
    "nvelles_col = [\"code_voie\", \"nums\", \"nb_nums\", \"type_voie_imm\", \"nom_voie_imm\", \"arrs_imm\"]\n",
    "\n",
    "df_ter[nvelles_col] = df_ter.apply(d_code_voie, axis=1, result_type='expand')\n",
    "\n",
    "df_ter['num_imm'] = df_ter['nums']\n",
    "df_ter = df_ter.explode('num_imm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 3 min de traitement (pour toutes les données)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes :\n",
    "- on ne prend pas en compte quand un numéro est 'bis' ou 'ter'. Mais ça devrait etre assez proche géographiquement pour pas poser de probleme in fine (surtout à l'échelle d'un quartier)\n",
    "- ce code ne prend pas en compte les \"1 à 5\" et \"numéros impairs\" parce qu'ils ne sont pas explicites dans les données, ce qui est dommage. (La raison est que c'est une opération très chronophage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# On créé la colonne \"nb_imms\" qui compte combien d'immeubles possède chaque personne au total\n",
    "def count_imms(row):\n",
    "    return len(df_ter.loc[df_ter.index_total == row.index_total])\n",
    "\n",
    "df_ter[\"nb_imms\"] = df_ter.apply(count_imms, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# un peu en dessous de 30 sec (pour toutes les données)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# On harmonise les colonnes pour utiliser les memes noms que dans les données de 1898\n",
    "\n",
    "dico_noms_colonnes = {'per' : 'nom_pers', 'prenom' : 'prenom_pers', 'statut' : 'civilite_pers',\n",
    "'num' : 'num_pers', 'type_voie' : 'type_voie_pers', 'nom_voie' : 'nom_voie_pers', 'arr' : 'arr_pers',\n",
    "'ville' : 'ville_pers', 'loc' : 'loc_pers', 'part' : 'part_pers',\n",
    "'code_voie' : 'code_voie_imm', 'nums' : 'liste_nums_imm', 'nb_nums' : 'nb_nums_imm'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_ter = df_ter.rename(mapper=dico_noms_colonnes, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 3 : définition et application d'une fonction pour le géocodage des adresses des immeubles, et des adresses des domiciles des propriétaires\n",
    "Nous allons géocoder nos données, c'est-à-dire associer à chaque adresse des coordonnées géographiques. Nous allons pour ce faire appliquer le géocodeur du gouvernement.\n",
    "\n",
    "Note : un géocodeur historique (développé par Bertrand Duménieu) est également disponible, et nous avons travaillé par ailleurs sur une comparaison entre les deux géocodeurs sur nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# fonction pour appliquer le géocodeur historique\n",
    "def mass_search(df, col_numero, col_type, col_nom, \n",
    "                *, \n",
    "                suffix, col_ville=None):\n",
    "    \"\"\"\n",
    "    appelle le géocodeur https://api-adresse.data.gouv.fr\n",
    "    et renvoie le dataframe avec de nouvelles informations :\n",
    "    latitude, longitude, ville et type de résultat de l'adresse trouvée\n",
    "    \n",
    "    Paramètres :\n",
    "      df :\n",
    "        le dataframe en entrée\n",
    "      col_numero :\n",
    "      col_type :\n",
    "      col_nom :\n",
    "        le nom des 3 colonnes qui contiennent le numéro, le type\n",
    "        de voie et le nom de la rue qui seront utilisées par le\n",
    "        géocodeur\n",
    "      col_ville :\n",
    "        ville dans laquelle le géocodeur va chercher, par défaut Paris\n",
    "      suffix :\n",
    "        est utilisé pour nommer les 4 colonnes en sorties\n",
    "        par exemple suffix=\"imm\" fera comme résultat :\n",
    "        lat_imm, lng_imm, result_type_imm et result_city_imm\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # constantes internes\n",
    "    dunder_indexname = f'__index_{suffix}__'\n",
    "    dunder_filename = '__search_data__.csv'\n",
    "\n",
    "    # cherche le nom de l'index ; en créé un si inexistant\n",
    "    if df.index.name is None:\n",
    "        df.index.name = dunder_indexname\n",
    "    indexname = df.index.name\n",
    "    \n",
    "    # vérifier que l'index est unique\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # créer le dataframe qu'on enverra au géocodeur\n",
    "    search_data = df[[col_numero, col_type, col_nom]]\n",
    "\n",
    "    # si on a pas de col_ville\n",
    "    col_ville = col_ville or 'Paris'\n",
    "    # on remplit la colonne ville\n",
    "    if col_ville in df.columns:\n",
    "        # si la colonne ville est une colonne existante\n",
    "        search_data['city'] = df[col_ville]\n",
    "    else:\n",
    "        # sinon, rajouter la constante (en général Paris)\n",
    "        search_data['city'] = col_ville\n",
    "        \n",
    "    # sauvegarder les informations pour les envoyer au géocodeur\n",
    "    search_data.to_csv(dunder_filename, index=False)\n",
    "    with open(dunder_filename) as feed:\n",
    "        result = requests.post(\n",
    "            \"https://api-adresse.data.gouv.fr/search/csv/\", \n",
    "            files={'data': feed},\n",
    "            data={'columns': search_data.columns})\n",
    "    # sauvegarder le résultat\n",
    "    result = pd.read_csv(io.StringIO(result.text))\n",
    "    \n",
    "    # on ne garde que ce qui nous intéresse\n",
    "    result = (result[['latitude', 'longitude', 'result_type', 'result_city']]\n",
    "              # on renomme pour respecter les suffixes\n",
    "              .rename(columns={\n",
    "                          'latitude': f'lat_{suffix}',\n",
    "                          'longitude': f'lng_{suffix}',\n",
    "                          'result_type': f'result_type_{suffix}',\n",
    "                          'result_city': f'result_city_{suffix}',\n",
    "                      }))\n",
    "    \n",
    "    # on fusionne le df initial et celui qu'on a obtenu du géocodeur\n",
    "    # et on restore l'index initial\n",
    "    return df.merge(result, left_index=True, right_index=True).set_index(indexname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47234/119662777.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  search_data['city'] = df[col_ville]\n",
      "/tmp/ipykernel_47234/119662777.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  search_data['city'] = col_ville\n"
     ]
    }
   ],
   "source": [
    "# Application de la fonction\n",
    "geoloc = mass_search(df_ter, 'num_pers', 'type_voie_pers', 'nom_voie_pers', suffix=\"pers\", col_ville='ville_pers')\n",
    "geoloc = mass_search(geoloc, 'num_imm', 'type_voie_imm', 'nom_voie_imm', suffix=\"imm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 14 min d'exécution (pour toutes les données)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 4 : Rajouter les colonnes de quartier\n",
    "Nous travaillons à l'échelle du quartier parisien (unité administrative), nous avons donc besoin d'associer un quartier à chaque adresse. Pour ce faire, nous chargeons les coordonnées des quartiers depuis Open Data Paris (cf. https://opendata.paris.fr/explore/dataset/quartier_paris/map/?disjunctive.c_ar&location=20,48.8932,2.36984&basemap=jawg.streets) dans Geopandas. Nous regardons ensuite dans quel quartier se trouvent les coordonnées trouvées par le géocodeur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. On commence par les adresses des immeubles parisiens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geopandas.geodataframe.GeoDataFrame"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On transforme 'geoloc' en un geodataframe\n",
    "geoloc['coord_imm'] = [Point(xy) for xy in zip(geoloc.lng_imm, geoloc.lat_imm)] \n",
    "geoloc = geopandas.GeoDataFrame(geoloc, geometry=geoloc.coord_imm) \n",
    "type(geoloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Import des donnees quartiers d'Open Data Paris\n",
    "csv_folder = os.path.join(parent_dir, 'donnees_quartiers')\n",
    "gdf_quartiers = geopandas.read_file(os.path.join(csv_folder, 'quartier_paris.shp'), encoding='utf-8')\n",
    "\n",
    "gdf_quartiers['c_qu'] = gdf_quartiers[\"c_qu\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaron/.local/lib/python3.10/site-packages/geopandas/geodataframe.py:2189: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\n",
      "Use `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n",
      "\n",
      "Left CRS: EPSG:4326\n",
      "Right CRS: None\n",
      "\n",
      "  return geopandas.sjoin(left_df=self, right_df=df, *args, **kwargs)  # noqa: B026\n"
     ]
    }
   ],
   "source": [
    "# On va chercher dans quel quartier se trouve chaque adresse d'immeuble\n",
    "df_av_quartiers = gdf_quartiers.sjoin(geoloc, predicate='contains', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Et on renomme la colonne du quartier de l'imm ainsi obtenue\n",
    "df_av_quartiers.rename(columns = {'c_qu':'quartier_imm'}, inplace = True)\n",
    "# On supprime les colonnes inutiles\n",
    "df_av_quartiers = df_av_quartiers.drop(['index_left', 'n_sq_qu', 'c_quinsee', 'l_qu', 'c_ar',\n",
    "       'n_sq_ar', 'perimetre', 'surface'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. On se concentre ensuite sur les adresses du domicile des propriétaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# On charge le dataframe qu'on vient d'obtenir, mais cette fois les coordonées qui nous intéressent sont celles des domiciles des propriétaires\n",
    "df_av_quartiers['coord_pers'] = [Point(xy) for xy in zip(df_av_quartiers.lng_pers, df_av_quartiers.lat_pers)]\n",
    "df_av_quartiers = geopandas.GeoDataFrame(df_av_quartiers, geometry=df_av_quartiers.coord_pers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaron/.local/lib/python3.10/site-packages/geopandas/geodataframe.py:2189: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\n",
      "Use `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n",
      "\n",
      "Left CRS: EPSG:4326\n",
      "Right CRS: None\n",
      "\n",
      "  return geopandas.sjoin(left_df=self, right_df=df, *args, **kwargs)  # noqa: B026\n"
     ]
    }
   ],
   "source": [
    "# Récupération de l'information quartier\n",
    "df_av_quartiers_2 = gdf_quartiers.sjoin(df_av_quartiers, predicate='contains', how='right')\n",
    "\n",
    "df_av_quartiers_2.rename(columns = {'c_qu':'quartier_pers'}, inplace = True)\n",
    "\n",
    "df_av_quartiers_2 = df_av_quartiers_2.drop(['index_left', 'n_sq_qu', 'c_quinsee', 'l_qu', 'c_ar',\n",
    "       'n_sq_ar', 'perimetre', 'surface'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Si on voulait faire un df avec que les propriétaires habitant dans Paris :\n",
    "# sub_sub_df = df_av_quartiers_2.dropna(subset=['quartier_pers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 5 : sauvegarde des dataframes en csv\n",
    "\n",
    "Les différents dataframes que nous venons de produire sont donc :\n",
    "- df_proprios > 1 ligne = 1 proprio (mais n'a pas le nombre total d'immeubles par personne)\n",
    "- df_bis > 1 ligne = 1 code. (n'a pas encore les bons noms de colonnes)\n",
    "- df_ter > 1 ligne = 1 immeuble\n",
    "- geoloc > apres l'entreprise de geolocalisation\n",
    "- df_av_quartiers > avec le quartier mais que de imm\n",
    "- df_av_quartiers_2 > avec les deux quartiers\n",
    "\n",
    "Nous nous intéressons donc surtout à `df_av_quartiers_2`.\n",
    "\n",
    "Nous allons dans un premier temps le manipulier pour obtenir un dataframe ou une ligne = un proprio, mais qui contient toutes les informations que nous avons associées au dataframe final. Nous allons dans un second temps exporter ces dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dataframe indexé par rapport aux proprios, mais avec toutes les informations utiles\n",
    "df_proprios_propre = df_av_quartiers_2.drop_duplicates('index_total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nous avons donc comme nombre total d'immeubles :\n",
      "- avant les traitements : 2118\n",
      "- apres les traitements : 2118\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nous avons donc comme nombre total d'immeubles :\\n- avant les traitements : {len(df_ter)}\\n- apres les traitements : {len(df_av_quartiers_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proprios_propre = df_proprios_propre[['index_total', 'nom_pers', 'prenom_pers', 'civilite_pers', 'org', 'num_pers',\n",
    "        'type_voie_pers', 'nom_voie_pers', 'arr_pers', 'ville_pers', 'loc_pers', 'part_pers', 'codes', 'nb_codes',\n",
    "        'nveaux_codes', 'nb_imms', 'lat_pers', 'lng_pers', 'result_type_pers', 'result_city_pers', 'geometry', 'coord_pers',\n",
    "        'quartier_imm', 'quartier_pers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes de df_proprios : 1051\n",
      "Nombre de lignes de df_bis : 1657\n",
      "Nombre de lignes de df_ter : 2118\n",
      "Nombre de lignes de geoloc : 2118\n",
      "Nombre de lignes de df_av_quartiers : 2118\n",
      "Nombre de lignes de df_av_quartiers_2 : 2118\n",
      "Nombre de lignes de df_proprios_propre : 1051\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nombre de lignes de df_proprios : {len(df_proprios)}\")\n",
    "print(f\"Nombre de lignes de df_bis : {len(df_bis)}\")\n",
    "print(f\"Nombre de lignes de df_ter : {len(df_ter)}\")\n",
    "print(f\"Nombre de lignes de geoloc : {len(geoloc)}\")\n",
    "print(f\"Nombre de lignes de df_av_quartiers : {len(df_av_quartiers)}\")\n",
    "print(f\"Nombre de lignes de df_av_quartiers_2 : {len(df_av_quartiers_2)}\")\n",
    "print(f\"Nombre de lignes de df_proprios_propre : {len(df_proprios_propre)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Définition des chemins utiles\n",
    "folder_path = os.path.join(parent_dir, 'sample_1951')\n",
    "folder_path_sauvegarde = os.path.join(folder_path, 'df_sauvegarde')\n",
    "\n",
    "os.mkdir(folder_path_sauvegarde)\n",
    "\n",
    "# Sauvegarde des dataframes\n",
    "\n",
    "# Les étapes intermédiaires \n",
    "df_proprios.to_csv(os.path.join(folder_path_sauvegarde, 'df_par_proprio_debut_traitement_1951.csv'))\n",
    "df_bis.to_csv(os.path.join(folder_path_sauvegarde, 'df_par_code_1951.csv'))\n",
    "df_ter.to_csv(os.path.join(folder_path_sauvegarde, 'df_par_num_avant_geoloc_1951.csv'))\n",
    "geoloc.to_csv(os.path.join(folder_path_sauvegarde, 'df_par_num_apres_geoloc_1951.csv'))\n",
    "df_av_quartiers.to_csv(os.path.join(folder_path_sauvegarde, 'df_par_num_av_quartier_imm_1951.csv'))\n",
    "\n",
    "# Et les deux tableaux finaux : le contenu de l'annuaire indexé par propriétaire, et le contenu de l'annaiare indexé par immeuble parisien\n",
    "df_av_quartiers_2.to_csv(os.path.join(folder_path, 'df_par_num_complet_1951.csv'))\n",
    "df_proprios_propre.to_csv(os.path.join(folder_path, 'df_par_proprio_complet_1951.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memoire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
